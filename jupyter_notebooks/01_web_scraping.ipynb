{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping - wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIKIPEDIA_URL = 'https://en.wikipedia.org'\n",
    "\n",
    "# Getting links for every movie on wikipedia from year 1990 to 2023 using BeautifulSoup.\n",
    "all_links = []\n",
    "for year in range(1990, 2024):\n",
    "    response = requests.get(WIKIPEDIA_URL + f'/wiki/List_of_American_films_of_{year}')\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        links = soup.select(selector='table > tbody > tr > td > i > a')\n",
    "        all_links.extend(links)\n",
    "    else:\n",
    "        print(\"Error retrieving the page:\", response.status_code)\n",
    "\n",
    "# Creating list of full links.\n",
    "movie_links = [WIKIPEDIA_URL + movie.get('href') for movie in all_links]\n",
    "\n",
    "# Deleting duplicates from the list of movie links.\n",
    "movie_links = list(set(movie_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting details of every movie from the list from wikipedia.\n",
    "all_movies = []\n",
    "errors = []\n",
    "for movie_link in movie_links:\n",
    "    response_wiki = requests.get(movie_link)\n",
    "    if response_wiki.status_code == 200:\n",
    "        movie_soup = BeautifulSoup(response_wiki.text, 'html.parser')\n",
    "        # Creating list of keys for dictionary.\n",
    "        labels = [label.getText() for label in movie_soup.select('th.infobox-label')]\n",
    "        labels.insert(0, 'Title')\n",
    "        # Values for dictionary.\n",
    "        records = [ record.getText() for record in movie_soup.select('td.infobox-data')]\n",
    "        records.insert(0, movie_link[30:])\n",
    "        # Creating dictionaries with data for every movie.\n",
    "        movie_dictionary = dict(zip(labels, records))\n",
    "        all_movies.append(movie_dictionary)\n",
    "    else:\n",
    "        print(f'Error {response_wiki.status_code} retrieving the page: {movie_link}' )\n",
    "        errors.append(movie_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving errors to .txt file.\n",
    "with open('../data/01_raw/errors.txt', 'w') as file:\n",
    "    for error in errors:\n",
    "        file.write(f'{error}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169\n"
     ]
    }
   ],
   "source": [
    "# Creating DataFrame from all dictionaries with raw informations about movies that need to be cleaned.\n",
    "raw_movies_df = pd.DataFrame(all_movies)\n",
    "\n",
    "# Checking how many columns are in df to evaluate if web scraping was done correctly.\n",
    "print(len(raw_movies_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title                      0\n",
       "Directed by               18\n",
       "Written by              3380\n",
       "Produced by              250\n",
       "Starring                 143\n",
       "Cinematography           480\n",
       "Edited by                372\n",
       "Music by                 435\n",
       "Productioncompany       6440\n",
       "Distributed by           288\n",
       "Release dates           4420\n",
       "Running time             110\n",
       "Country                 1927\n",
       "Language                 871\n",
       "Budget                  2457\n",
       "Box office              1364\n",
       "Screenplay by           5083\n",
       "Story by                6877\n",
       "Productioncompanies     2871\n",
       "Release date            4075\n",
       "Countries               6631\n",
       "Released                7470\n",
       "Genre                   7563\n",
       "Length                  7675\n",
       "Label                   7493\n",
       "Producer                7869\n",
       "Languages               7665\n",
       "Based on                5065\n",
       "Theme music composer    8273\n",
       "Country of origin       8139\n",
       "dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are way too many columns. Some movies had too catch keys that are useless.\n",
    "# Let's see in which place usefull columns end by checking number of missing values.\n",
    "raw_movies_df.isnull().sum().head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useless columns start after 'based on'.\n",
    "# Removing useless columns.\n",
    "raw_movies_df = raw_movies_df.iloc[:, list(range(21)) + [27]]\n",
    "\n",
    "# Also let's remove all movies that haven't got information on box office.\n",
    "# This columns is the target of this supervised learning project so records without it are also useless.\n",
    "# I'm doing this already here to decrease number of records that I will scrape data from rottentomatoes.com for.\n",
    "# The reason of missing data for most of these movies is simple - they weren't screened in cinema, only on DVD,\n",
    "# TV or streaming platforms.\n",
    "raw_movies_df.dropna(subset=['Box office'], inplace=True)\n",
    "\n",
    "# More in depths data cleansing will be done after splitting dataset into train and test sets.\n",
    "\n",
    "# Saving wikipedia DataFrame to csv.\n",
    "raw_movies_df.to_csv('../data/01_raw/wikipedia.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping - rottentomatoes.com "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title\n",
       "_(film)    1801\n",
       "0_film)     193\n",
       "2_film)     190\n",
       "9_film)     180\n",
       "1_film)     180\n",
       "           ... \n",
       "_Served       1\n",
       "ecticut       1\n",
       "ms_Club       1\n",
       ":_Miami       1\n",
       "Sicko         1\n",
       "Name: count, Length: 2899, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing links for rottentomatoes.com from where I will scrape the rest of the data needed for this project.\n",
    "# Many titles from wikipedia ends with _(film). I need to remove this phrase.\n",
    "raw_movies_df['Title'].str[-7:].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title_rotten\n",
       "_(2019_film)    70\n",
       "_(1996_film)    66\n",
       "_(2010_film)    66\n",
       "_(2009_film)    63\n",
       "_(2008_film)    60\n",
       "                ..\n",
       "Margin_Call      1\n",
       "Late_Quartet     1\n",
       "Never_Sleeps     1\n",
       "Mystery_Team     1\n",
       "Devils_Due       1\n",
       "Name: count, Length: 5163, dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patterns_to_replace = {\n",
    "        r'_\\(film\\)$': '',\n",
    "        r'%27': '',\n",
    "        r'[:.!?,]': '',\n",
    "        r'%26': 'and',\n",
    "        r'-': '_'\n",
    "    }\n",
    "\n",
    "# Formatting title in the way that is used on rottentomatoes.com. Removing and replacing characters.\n",
    "# Removing phrase '_(film)' from the end of the titles.\n",
    "for pattern, replacement in patterns_to_replace.items():\n",
    "    raw_movies_df['Title_rotten'] = raw_movies_df['Title_rotten'].str.replace(pattern, replacement, regex=True)\n",
    "\n",
    "# Also in many cases on wikipedia if there are couple of movies with the same title there is addition of the year in the end.\n",
    "raw_movies_df['Title_rotten'].str[-12:].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title_rotten\n",
       "_2019    70\n",
       "_2010    66\n",
       "_1996    66\n",
       "_2009    63\n",
       "_2008    60\n",
       "         ..\n",
       "let_2     1\n",
       "_Tall     1\n",
       "id90s     1\n",
       "llion     1\n",
       "s_Due     1\n",
       "Name: count, Length: 3310, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On rottentomatoes instead of for example '_(2019_film)' I need '_2019'.\n",
    "# Also for some of the movies it will probably be good to check option without the year if the link with year won't be found.\n",
    "# So I need 2 columns with titles to scrape data from rottentomatoes.\n",
    "\n",
    "# Creating column without the year.\n",
    "raw_movies_df['Title_no_year'] = raw_movies_df['Title_rotten'].str.replace(r'_\\(\\d{4}_film\\)$', '', regex=True)\n",
    "\n",
    "# Creating column with _dddd format of the year.\n",
    "raw_movies_df['Title_rotten'] = raw_movies_df['Title_rotten'].str.replace(r'(_)\\((\\d{4})_film\\)$', r'\\1\\2', regex=True)\n",
    "\n",
    "# Checking if the changes were done correctly.\n",
    "raw_movies_df['Title_rotten'].str[-5:].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title_no_year\n",
       "_the_End_of_the_World    4\n",
       "_(2015_American_film)    4\n",
       "_(2016_American_film)    4\n",
       "_(2010_American_film)    3\n",
       "he_Planet_of_the_Apes    3\n",
       "                        ..\n",
       "Moonlight_Mile           1\n",
       "Child_44                 1\n",
       "Letters_to_Juliet        1\n",
       "Wonder_Park              1\n",
       "Loverboy                 1\n",
       "Name: count, Length: 6815, dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if there is anything else to remove.\n",
    "raw_movies_df['Title_no_year'].str[-21:].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are couple of films ending with pattern '_(dddd_American_film)' that also needs to be changed.\n",
    "\n",
    "# Column without the year.\n",
    "raw_movies_df['Title_no_year'] = raw_movies_df['Title_no_year'].str.replace(r'_\\(\\d{4}_American_film\\)$', '', regex=True)\n",
    "\n",
    "# Column with _dddd format of the year.\n",
    "raw_movies_df['Title_rotten'] = raw_movies_df['Title_rotten'].str.replace(r'(_)\\((\\d{4})_American_film\\)$', r'\\1\\2', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Title_no_year\n",
       "The_    1324\n",
       "Amer      30\n",
       "Love      29\n",
       "Star      25\n",
       "Blac      25\n",
       "        ... \n",
       "Anom       1\n",
       "Slin       1\n",
       "T%C3       1\n",
       "Lass       1\n",
       "Pric       1\n",
       "Name: count, Length: 2500, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After running the program I discovered that if a movie title starts with 'the' it is often removed from rottentomatoes https address.\n",
    "# So I need to take this into account while scraping data.\n",
    "raw_movies_df['Title_no_year'].str[:4].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating 2 new columns with links without 'the'.\n",
    "raw_movies_df['Title_no_the'] = raw_movies_df['Title_rotten'].str.replace(r'^The_', '', regex=True)\n",
    "raw_movies_df['Title_no_the_no_year'] = raw_movies_df['Title_no_year'].str.replace(r'^The_', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After running program another time I discovered that 'A_' is often skipped in titles.\n",
    "# Creating new columns.\n",
    "raw_movies_df['Title_no_a'] = raw_movies_df['Title_rotten'].str.replace(r'^A_', '', regex=True)\n",
    "raw_movies_df['Title_no_a_no_the'] = raw_movies_df['Title_no_the'].str.replace(r'^A_', '', regex=True)\n",
    "raw_movies_df['Title_no_a_no_year'] = raw_movies_df['Title_no_year'].str.replace(r'^A_', '', regex=True)\n",
    "raw_movies_df['Title_no_a_no_the_no_year'] = raw_movies_df['Title_no_the_no_year'].str.replace(r'^A_', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting details of every movie from rottentomatoes.com\n",
    "\n",
    "ROTTENTOMATOES_URL = 'https://www.rottentomatoes.com/'\n",
    "all_movies_rotten = []\n",
    "\n",
    "# Function to scrape data from rottentomatoes.\n",
    "def scrape_rotten(response, column_name):\n",
    "    rotten_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    try:\n",
    "        year = rotten_soup.select_one('p.info[data-qa=\"score-panel-subtitle\"]').getText().strip()\n",
    "        reviews_score =  rotten_soup.select_one('score-board-deprecated').get('tomatometerscore')\n",
    "        ratings_score =  rotten_soup.select_one('score-board-deprecated').get('audiencescore')\n",
    "        reviews_count = rotten_soup.select_one('a[data-qa=\"tomatometer-review-count\"]').getText().strip()\n",
    "        ratings_count = rotten_soup.select_one('a[data-qa=\"audience-rating-count\"]').getText().strip()\n",
    "        movie_synopsis = rotten_soup.select_one('p[data-qa=\"movie-info-synopsis\"]').getText().strip()\n",
    "        # Creating list of keys for dictionary.\n",
    "        labels = [label.getText().strip() for label in rotten_soup.select('b.info-item-label')]\n",
    "        labels.extend(['Year', 'Reviews_score', 'Ratings_score', 'Reviews_count', 'Ratings_count', 'Synopsis'])\n",
    "        # Creating list of values for dictionary.\n",
    "        records = [record.getText().strip() for record in rotten_soup.select('span[data-qa=\"movie-info-item-value\"]')]\n",
    "        records.extend([year, reviews_score, ratings_score, reviews_count, ratings_count, movie_synopsis])\n",
    "    # If any of the object in rottentomatoes page won't be found then function returns error.\n",
    "    # In most cases this will happen when scraping a movie from the wrong year but with the same title.\n",
    "    except AttributeError:\n",
    "        labels = ['Error - scraping']\n",
    "        records = [ROTTENTOMATOES_URL + row[column_name]]\n",
    "        print(f'Error scraping the page: {ROTTENTOMATOES_URL + row[column_name]}')\n",
    "    return labels, records\n",
    "\n",
    "# If link from column 'Title_rotten' won't be found then links from columns without 'the', 'year' and 'a' should be\n",
    "# tried before recognizing row as Error.\n",
    "title_columns = ['Title_rotten', 'Title_no_the', 'Title_no_a', 'Title_no_a_no_the', 'Title_no_year', 'Title_no_the_no_year', \n",
    "                 'Title_no_a_no_year', 'Title_no_a_no_the_no_year']\n",
    "\n",
    "# Iterating through all rows in dataframe.\n",
    "for i, row in raw_movies_df.iterrows():\n",
    "    accessed_the_page = False\n",
    "    checked_titles = []\n",
    "    # Iterating through all columns with titles.\n",
    "    for title_column in title_columns:\n",
    "        current_title = row[title_column]\n",
    "        # Variables current_title and checked_titles are used to prevent attempting to connect to website using identical title.\n",
    "        # It may often happen that for example value in 'Title_rotten' and 'Title_no_the' is the same.\n",
    "        # In that case making requests for both of them is unnecessary.\n",
    "        if current_title not in checked_titles:\n",
    "            checked_titles.append(current_title)\n",
    "            response = requests.get(ROTTENTOMATOES_URL + 'm/' + row[title_column])\n",
    "            if response.status_code == 200:\n",
    "                labels, records = scrape_rotten(response, title_column)\n",
    "                accessed_the_page = True\n",
    "                if labels != ['Error - scraping']:\n",
    "                    break\n",
    "    # Adding column 'Error' so the rows that haven't managed to reach connection wouldn't be skipped in dataframe.\n",
    "    if not(accessed_the_page):\n",
    "        labels = ['Error']\n",
    "        records = [ROTTENTOMATOES_URL + row['Title_rotten']]\n",
    "        print(f'Error {response.status_code} retrieving the page: {ROTTENTOMATOES_URL + row[\"Title_rotten\"]}')\n",
    "    # Creating dictionaries with data for every movie.\n",
    "    movie_dictionary = dict(zip(labels, records))\n",
    "    all_movies_rotten.append(movie_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving rottentomatoes DataFrame to csv.\n",
    "rotten_movies_df = pd.DataFrame(all_movies_rotten)\n",
    "rotten_movies_df.to_csv('../data/01_raw/rotten_tomatoes.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial data cleansing for further web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing raw data.\n",
    "raw_wikipedia = pd.read_csv('../data/01_raw/wikipedia.csv')\n",
    "rotten_tomatoes = pd.read_csv('../data/01_raw/rotten_tomatoes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error               531\n",
      "Error - scraping    411\n",
      "dtype: int64\n",
      "Rows: 6940\n"
     ]
    }
   ],
   "source": [
    "# Checking for how many films for rottentomatoes there was problem with scraping the data.\n",
    "print(rotten_tomatoes[['Error', 'Error - scraping']].notnull().sum())\n",
    "print(f'Rows: {len(rotten_tomatoes)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Release dates</th>\n",
       "      <th>Release date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nMarch 14, 1990 (1990-03-14) (Baltimore)\\nApril 6, 1990 (1990-04-06) (United States)\\n</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n1 December 1993 (1993-12-01) (France)\\n[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nMay 20, 1999 (1999-05-20) (Cannes)\\nJuly 2, 1999 (1999-07-02) (United States)\\n</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nDecember 22, 1999 (1999-12-22)\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n1 May 1998 (1998-05-01) (United States)\\n20 November 1998 (1998-11-20) (United Kingdom)\\n24 December 1998 (1998-12-24) (Germany)\\n</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nAugust 9, 1991 (1991-08-09)\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\nMay 11, 1996 (1996-05-11) (Cannes)\\nNovember 29, 1996 (1996-11-29) (UK)\\nMay 16, 1997 (1997-05-16) (United States)\\n</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\nMay 11, 1996 (1996-05-11) (Directors' Fortnight)\\nOctober 11, 1996 (1996-10-11) (United States)\\n</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nMay 27, 1998 (1998-05-27)\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nJune 12, 1998 (1998-06-12)\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\\nSeptember 14, 1993 (1993-09-14) (TIFF)\\nSeptember 29, 1993 (1993-09-29) (United States)\\n</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nNovember 17, 1993 (1993-11-17) (United States)\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nMay 25, 1977 (1977-05-25)\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\\nJanuary 21, 1998 (1998-01-21) (Sundance)\\nMay 17, 1998 (1998-05-17) (Cannes)\\nJune 12, 1998 (1998-06-12) (United States)\\n</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\\nJuly 27, 1998 (1998-07-27) (Los Angeles)\\nAugust 5, 1998 (1998-08-05) (United States)\\n</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                           Release dates  \\\n",
       "0                                                \\nMarch 14, 1990 (1990-03-14) (Baltimore)\\nApril 6, 1990 (1990-04-06) (United States)\\n   \n",
       "1                                                                                                                                    NaN   \n",
       "2                                                      \\nMay 20, 1999 (1999-05-20) (Cannes)\\nJuly 2, 1999 (1999-07-02) (United States)\\n   \n",
       "3                                                                                                                                    NaN   \n",
       "4   \\n1 May 1998 (1998-05-01) (United States)\\n20 November 1998 (1998-11-20) (United Kingdom)\\n24 December 1998 (1998-12-24) (Germany)\\n   \n",
       "5                                                                                                                                    NaN   \n",
       "6                 \\nMay 11, 1996 (1996-05-11) (Cannes)\\nNovember 29, 1996 (1996-11-29) (UK)\\nMay 16, 1997 (1997-05-16) (United States)\\n   \n",
       "7                                    \\nMay 11, 1996 (1996-05-11) (Directors' Fortnight)\\nOctober 11, 1996 (1996-10-11) (United States)\\n   \n",
       "8                                                                                                                                    NaN   \n",
       "9                                                                                                                                    NaN   \n",
       "10                                           \\nSeptember 14, 1993 (1993-09-14) (TIFF)\\nSeptember 29, 1993 (1993-09-29) (United States)\\n   \n",
       "11                                                                                                                                   NaN   \n",
       "12                                                                                                                                   NaN   \n",
       "13          \\nJanuary 21, 1998 (1998-01-21) (Sundance)\\nMay 17, 1998 (1998-05-17) (Cannes)\\nJune 12, 1998 (1998-06-12) (United States)\\n   \n",
       "14                                             \\nJuly 27, 1998 (1998-07-27) (Los Angeles)\\nAugust 5, 1998 (1998-08-05) (United States)\\n   \n",
       "\n",
       "                                          Release date  \n",
       "0                                                  NaN  \n",
       "1         \\n1 December 1993 (1993-12-01) (France)\\n[1]  \n",
       "2                                                  NaN  \n",
       "3                   \\nDecember 22, 1999 (1999-12-22)\\n  \n",
       "4                                                  NaN  \n",
       "5                      \\nAugust 9, 1991 (1991-08-09)\\n  \n",
       "6                                                  NaN  \n",
       "7                                                  NaN  \n",
       "8                        \\nMay 27, 1998 (1998-05-27)\\n  \n",
       "9                       \\nJune 12, 1998 (1998-06-12)\\n  \n",
       "10                                                 NaN  \n",
       "11  \\nNovember 17, 1993 (1993-11-17) (United States)\\n  \n",
       "12                       \\nMay 25, 1977 (1977-05-25)\\n  \n",
       "13                                                 NaN  \n",
       "14                                                 NaN  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data for 942 out of 6940 wasn't scraped for rottentomatoes.com.\n",
    "# The main reason is that on rottentomatoes.com for many films there are random numbers added before the title.\n",
    "# For example: https://www.rottentomatoes.com/m/10011582-tron_legacy.\n",
    "# Those numbers doesn't follow any visible pattern so scraping those movies by title is impossible.\n",
    "# However there is different way to approach this problem.\n",
    "# I can find hyperlinks for these movies on profile pages of their directors on rottentomatoes.com.\n",
    "# For that I will need to clean columns in wikipedia dataframe that contain information about director and about\n",
    "# release date of the movies.\n",
    "\n",
    "# Let's take a look at release dates columns.\n",
    "pd.set_option('display.max_colwidth', 140)\n",
    "raw_wikipedia[['Release dates', 'Release date']].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakub\\AppData\\Local\\Temp\\ipykernel_14404\\3897829050.py:8: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  raw_wikipedia['Release dates'][~raw_wikipedia['Release dates'].str.contains('(United States)|(US)|(U.S.)') & (raw_wikipedia['Release dates'] != '')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24                  \\nDecember 23, 1994 (1994-12-23)\\n(limited)\\nJanuary 13, 1995 (1995-01-13)\\n(North America)\n",
       "45      \\nSeptember 24, 1999 (1999-09-24) (Sundance Film Festival)\\nSeptember 21, 2001 (2001-09-21) (Germany)\\n\n",
       "52                                            \\nMay 26, 1995 (1995-05-26) (Cannes)\\nMay 10, 1996 (1996-05-10)\\n\n",
       "66                                          \\nApril 23, 1999 (North America)\\nApril 30, 1999 (United Kingdom)\\n\n",
       "112                                       \\nSeptember 7, 1997 (1997-09-07) (TIFF)\\nJune 19, 1998 (1998-06-19)\\n\n",
       "                                                         ...                                                   \n",
       "6741                            \\n8 August 2011 (2011-08-08) (Locarno)\\n11 September 2011 (2011-09-11) (TIFF)\\n\n",
       "6779                                 \\n7 September 2007 (2007-09-07) (Venice)\\n20 September 2007 (2007-09-20)\\n\n",
       "6830                      \\n12 February 2008 (2008-02-12) (BIFF)\\n18 April 2008 (2008-04-18) (United Kingdom)\\n\n",
       "6889                              \\n24 May 2009 (2009-05-24) (Cannes)\\n30 December 2009 (2009-12-30) (France)\\n\n",
       "6935                                 \\n9 September 2012 (2012-09-09) (TIFF)\\n1 January 2013 (2013-01-01) (UK)\\n\n",
       "Name: Release dates, Length: 410, dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Release dates in wikipedia are split perfectly between 2 columns (when we add non-null records it gives 6940)\n",
    "# depending whether the movie had one or more premieres.\n",
    "# I need to remove places of premiere from both columns and chose only one date from 'Release dates' before I will be\n",
    "# able to format columns to datetime format.\n",
    "\n",
    "# Checking if all 'Release dates' have premiere in United States.\n",
    "raw_wikipedia.fillna({'Release dates': ''}, inplace=True)\n",
    "raw_wikipedia['Release dates'][~raw_wikipedia['Release dates'].str.contains('(United States)|(US)|(U.S.)') & (raw_wikipedia['Release dates'] != '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It appears that 410 rows don't have premiere in United States. So I won't be able to choose date this way.\n",
    "# I decided to always choose the first date of the premiere because it fits the best to rottentomatoes.\n",
    "# Unfortunately dates are saved in different formats: some are saved like that 11 May 1990 or that February 27, 1998\n",
    "# and some even have only month and year. So first I need to transform all of these dates to ('yyyy-mm-dd').\n",
    "\n",
    "wikipedia = raw_wikipedia.copy()\n",
    "\n",
    "# Regular expressions to find dates in all formats.\n",
    "date_patterns = [\n",
    "    r'(\\d{4}-\\d{2}-\\d{2})',\n",
    "    r'(\\d{1,2} (?:January|February|March|April|May|June|July|August|September|October|November|December) \\d{4})',\n",
    "    r'((?:January|February|March|April|May|June|July|August|September|October|November|December) \\d{1,2}, \\d{4})',\n",
    "    r'((?:January|February|March|April|May|June|July|August|September|October|November|December) \\d{1,2}(?:st|nd|rd|th), \\d{4})',\n",
    "    r'(\\d{4}-\\d{2}(?!\\-\\d\\d))'\n",
    "]\n",
    "\n",
    "# Function to extract dates and format them to datetime format.\n",
    "def extract_date(df, column_name, patterns=date_patterns):\n",
    "    selected_dates = None\n",
    "     # I have to go through each pattern from dictionary separately otherwise function to_datetime() wouldn't know what to do.\n",
    "    for pattern in patterns:\n",
    "        # .first() to always choose the first date in the column.\n",
    "        dates_of_next_type = df[column_name].str.extractall(pattern).groupby(level=0).first()\n",
    "        dates_of_next_type = pd.to_datetime(dates_of_next_type[0]).to_frame()\n",
    "        if selected_dates is None:\n",
    "            selected_dates = dates_of_next_type\n",
    "        else:\n",
    "            # .min() to always choose the first date from 2 Series.\n",
    "            selected_dates = pd.concat([selected_dates, dates_of_next_type], axis=1).min(axis=1)\n",
    "    return selected_dates\n",
    "\n",
    "# Extracting single dates for: 'Release dates' and 'Release date columns'.\n",
    "wikipedia['Main release date'] = extract_date(wikipedia, 'Release dates')\n",
    "wikipedia['Main release date 2'] = extract_date(wikipedia,'Release date')\n",
    "\n",
    "# Combining both columns together.\n",
    "wikipedia['Main release date'] = wikipedia['Main release date'].combine_first(wikipedia['Main release date 2'])\n",
    "\n",
    "# Creating column that contains only year. It will be needed for scraping data.\n",
    "wikipedia['Year - wikipedia'] = wikipedia['Main release date'].dt.year\n",
    "\n",
    "# Removing old columns:\n",
    "wikipedia.drop(['Main release date 2', 'Release date', 'Release dates'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing 'Directed by' column to web scraping.\n",
    "\n",
    "patterns_to_replace = {\n",
    "    '\\n\\n': '',\n",
    "    r'Supervising Director[s]?\\n': '',\n",
    "    r'\\(.*\\)': '',\n",
    "    r'\\[.*\\]': '',\n",
    "    ', Jr.': ' Jr.',\n",
    "    '\\n': ',',\n",
    "    # In some cases directors aren't split at all. I recognize these cases by finding small and large letters next to each other.\n",
    "    # I'm making sure not to split names like 'McAdams' by adding condition that there can't be large-small-large combination.\n",
    "    r'[^A-Z]([a-z\\.ąćęłńóśźżäöüñéáò])([A-Z])': r'\\1, \\2'\n",
    "}\n",
    "\n",
    "# Adding ', ' between different options of many directors in one field and removing unnecessary descriptions.\n",
    "for pattern, replacement in patterns_to_replace.items():\n",
    "    wikipedia['Directed by'] = wikipedia['Directed by'].str.replace(pattern, replacement, regex=True)\n",
    "\n",
    "wikipedia['Directed by'] = wikipedia['Directed by'].str.strip()\n",
    "\n",
    "# Splitting 'Directed by' column to 2 columns.\n",
    "# Usually there will be only 1 director but sometimes there are 2 importent directors like for example Russo brothers.\n",
    "# If there are more than 2 directors I remove them because they won't be importent in analysis.\n",
    "directed_by_split = wikipedia['Directed by'].str.split(',', n=2, expand=True)\n",
    "wikipedia[['Director_1', 'Director_2']] = directed_by_split.iloc[:, :2]\n",
    "wikipedia.drop('Directed by', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting column in rottentomatoes that contains year and different informations.\n",
    "# Leaving only year because genre and runtime are already contained in different columns.\n",
    "rotten_tomatoes['Year'] = rotten_tomatoes['Year'].str.split(',', expand=True).iloc[:, 0]\n",
    "rotten_tomatoes['Year'] = rotten_tomatoes['Year'].fillna(0).astype('int32')\n",
    "\n",
    "# Concatinating data into one dataframe.\n",
    "all_movies = pd.concat([wikipedia, rotten_tomatoes], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrect scraping: 210\n"
     ]
    }
   ],
   "source": [
    "# Checking if the previous scraping was correct by comparing release years of wikipedia and rottentomatoes.\n",
    "# Difference of 1 year will be allowed because such difference could result from considering different premiere.\n",
    "# If the difference is bigger than that then I will remove uncorrectly scraped data.\n",
    "mask = (abs(all_movies['Year - wikipedia'] - all_movies['Year']) > 1) & (all_movies['Year'] != 0)\n",
    "print(f'Incorrect scraping: {len(all_movies.loc[mask, :])}')\n",
    "all_movies.loc[mask, 'Rating:':] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping - rottentomatoes.com  - method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting links to rottentomatoes movies from directors of these movies.\n",
    "\n",
    "all_movies['movie_link_from_director'] = ''\n",
    "missing_movies_df = all_movies.loc[all_movies['Rating:'].isna()]\n",
    "errors_directors = []\n",
    "\n",
    "# Iterating through all rows in dataframe that are missing information from rottentomatoes.\n",
    "for i, row in missing_movies_df.iterrows():\n",
    "    director_link = ROTTENTOMATOES_URL + 'celebrity/' + row['Director_1'].replace(' ', '_')\n",
    "    response_director = requests.get(director_link)\n",
    "    if response_director.status_code == 200:\n",
    "        director_soup = BeautifulSoup(response_director.text, 'html.parser')\n",
    "        # Looking for the movie that was directed in the specific year.\n",
    "        parent_elements = director_soup.select(f'tr[data-year$=\"{row[\"Year - wikipedia\"]}\"]')\n",
    "        movie_links = []\n",
    "        for parent in parent_elements:\n",
    "            # Checking if the person was actually Director of the found movie.\n",
    "            # It is necessary because he could have a different role like for example Producer.\n",
    "            check_role = parent.select_one('td.celebrity-filmography__credits').getText().strip().split(',')\n",
    "            if 'Director' in check_role:\n",
    "                movie_links.append(parent.select_one('td.celebrity-filmography__title a').get('href'))\n",
    "        # If the person directed more than 1 movie in given year then I won't risk assigning a link to the wrong movie.\n",
    "        if len(movie_links) == 1:\n",
    "            missing_movies_df.at[i, 'movie_link_from_director'] = movie_links[0]\n",
    "    else:\n",
    "        print(f'Error {response_director.status_code} retrieving the page: {director_link}' )\n",
    "        errors_directors.append(director_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving errors to .txt file.\n",
    "with open('../data/01_raw/errors_directors.txt', 'wb') as file:\n",
    "    for error in errors_directors:\n",
    "        file.write(error.encode('utf-8') + b'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping remaining movies from rottentomatoes.com by directors.\n",
    "\n",
    "movies_to_scrape = missing_movies_df.loc[missing_movies_df['movie_link_from_director'] != '', :]\n",
    "new_movies_list = []\n",
    "\n",
    "# Iterating through all rows in dataframe that are missing information from rottentomatoes using new links.\n",
    "for i, row in movies_to_scrape.iterrows():\n",
    "    new_movie_link = ROTTENTOMATOES_URL + row['movie_link_from_director']\n",
    "    response_new_movie = requests.get(new_movie_link)\n",
    "    if response_new_movie.status_code == 200:\n",
    "        # Scraping data for the movie.\n",
    "        labels, records = scrape_rotten(response_new_movie, 'movie_link_from_director')\n",
    "    else:\n",
    "        labels = ['Error']\n",
    "        records = [new_movie_link]\n",
    "        print(f'Error {response_new_movie.status_code} retrieving the page: {new_movie_link}')\n",
    "    # Creating dictionaries with data for every movie.\n",
    "    new_movie_dictionary = dict(zip(labels, records))\n",
    "    new_movies_list.append(new_movie_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe from newly scraped data.\n",
    "new_movies_df = pd.DataFrame(new_movies_list)\n",
    "new_movies_df.index = movies_to_scrape.index\n",
    "movies_to_scrape.loc[:, 'Rating:':'Aspect Ratio:'] = new_movies_df.loc[:, :'Aspect Ratio:']\n",
    "movies_to_scrape.loc[:, 'Error - scraping'] = new_movies_df.loc[:, 'Error - scraping']\n",
    "# Assigning newly scraped data to the main dataframe on the correct indexes.\n",
    "all_movies.loc[movies_to_scrape.index, :] = movies_to_scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping - wikipedia.org #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# During model development process I discovered that I'm missing key feature that could be very good indicator of the movie results.\n",
    "# This feature being the knowledge which movies are sequels - such information could be crucial, because I could use previous movie \n",
    "# box office to predict its sequel. I hoped that these information will be found in Synopsis on rottentomatoes but in most cases it wasn't\n",
    "# so this is why I scrape this additional data from wikipedia.\n",
    "\n",
    "all_movies['Description'] = ''\n",
    "\n",
    "# Iterating through all movies.\n",
    "for i, row in all_movies.iterrows():\n",
    "    movie_link = WIKIPEDIA_URL +  '/wiki/' + row['Title']\n",
    "    response_movie = requests.get(movie_link)\n",
    "    if response_movie.status_code == 200:\n",
    "        # Scraping description of the movie that will be later used to extract the title of previous movie in the franchise/series.\n",
    "        movie_soup = BeautifulSoup(response_movie.text, 'html.parser')\n",
    "        all_movies.at[i, 'Description'] = movie_soup.select_one('div.mw-body-content div p:not(.mw-empty-elt)')\n",
    "    else:\n",
    "        print(f'Error {response_movie.status_code} retrieving the page: {movie_link}' )\n",
    "\n",
    "# Formating to string to enable searching with regex.\n",
    "all_movies['Description'] = all_movies['Description'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving final dataframe to file.\n",
    "all_movies.to_csv('../data/01_raw/all_movies.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
